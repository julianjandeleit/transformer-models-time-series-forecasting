\documentclass[11pt,a4paper]{article}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage[authoryear]{natbib}
\usepackage{url}


\title{Evaluating Time-Series Transformers for Forecasting Plant-Based Sensor Data}
\author{Julian Jandeleit}
\date{\today}

\begin{document}
\maketitle

%TODO: citation for parts of abstract / same content referenced later in inrtoduction
\begin{abstract}
\noindent
%\begin{itemize}
%    \item Time-series transformers
%    \item Influenza-like Illness (ILI)
%    \item Plant-based sensors
%    \item Forecasting
%    \item Sparse data
%    \item Findings
%\end{itemize}
Physiological data in biological systems can have enough information to infer external conditions in the environment.
To use plants for monitoring the environment, their sensors energy consumption needs to be minimized.
One possibility is, to replace data transmission by forecasting the data and only transmitting if necessary.

Transformers showed good capability in large language models.
\citet{wu_2020} suggests they can also be useful in forecasting time-series data.
This seminar paper summarizes transformers for time-series forecasting as done by \citet{wu_2020} and evaluates their performance in the context of forecasting plant-based sensor data.
We find that they show potential to be used in this scenario, especially if further optimization to the task is done.
\end{abstract}

%TODO what to cite? Details missiing, anything unnecessary?
\section{Introduction}
% \noindent
% \begin{itemize}
%     \item Background on plant-based sensors and their applications
%     \item Challenges in time-series data forecasting for plant-based sensors
%     \item Objective of the study
%     \item Overview of the adaptation of an existing ILI forecasting model
% \end{itemize}
Plant-based sensors have emerged as a promising approach for monitoring environmental conditions using physiological data from plants. 
These sensors leverage the inherent ability of plants to respond to external factors, such as light, temperature, humidity, and pollutants, providing valuable insights into the surrounding environment. 
By monitoring plants' physiological responses, it becomes possible to infer crucial information about the ecosystem and make informed decisions for environment protection.

However, one of the key challenges in using plant-based sensors lies in the energy consumption of these sensors. 
To ensure long-term monitoring without exhausting the sensor's energy resources, it is essential to minimize data transmission and optimize energy usage. 
One strategy to achieve this is by forecasting the sensor data, allowing for selective transmission only when prediction if off by some threshold. 
By accurately predicting the future values of the plant-based sensor data, it becomes possible to rely on the forecast for a longer time, thus reducing unnecessary transmissions and preserving energy.
%TODO: larger captions, description
\begin{figure}%[!htb]
    \centering
    \includegraphics[width=\linewidth]{figures/img_transpiration_day.png}
    \caption{Typical day for studied datasets}
    \label{fig:example_datasets}
\end{figure}
~\\
Forecasting time-series data from plant-based sensors, however, presents unique challenges. 
The data generated by these sensors is often characterized by complex temporal dependencies and non-linear patterns as shown in Fig.~\ref{fig:example_datasets}. 
In recent years, transformers have shown remarkable success in various natural language processing tasks, demonstrating their ability to capture long-range dependencies and model complex sequential data. 
While initially designed for language processing, transformers have also shown promise in time-series forecasting tasks. \citet{wu_2020} proposed a transformer-based model for time-series forecasting, highlighting its potential for accurate predictions.
Their work focuses on forecasting influenza-like-illnesses (ILI).

~\\
In the following sections, we summarize their approach for ILI-forecasting and adapt their time-series transformer to the context of forecasting plant-based sensor data.
Finally, we evaluate their performance and assess their feasibility.

% This part necessary?
% \section{Time-Series Forecasting using Transformers}
% \subsection{Overview of Approaches}
% \begin{itemize}
%     \item General overview of approaches
%     \item Time series forecasting with Transformers
%     \item Basic Classical approaches
%     \item Key Differences
% \end{itemize}
% \subsection{The ILI-Prevalence Case}
\section{The ILI-Prevalence Case} % TODO: how to structure with subsections? What depth is necessary?
\label{sec:ili-prevalence-case}
% \begin{itemize}
% \item Time-series transformers for ILI data Forecasting
% \item Model description and difference to normal transformers
% \item Appication, Success and Differences
% \end{itemize}
\citet{wu_2020} present different traditional forecasting methods in the field of influenza like illness monitoring.
They argue that transformers can improve forecasting results because the history can be processed as a whole.
This stays in contrast with traditional sequence-aligned models, that process the input in ordered sequences.
They state this gives transformers the potential to model
dynamics of time series data that sequence models find difficult.

~\\ % paragraph
\citet{wu_2020} adapt transformers to ILI forecasting and compare them to traditional sequence-aligned models. 
The general architecture stays similar to \citet{Vaswani2017}. 
The key differences are due to the nature of the data the transformer should be applied to, i.e., text vs. numerical time-series features.
The general structure is visualized in Fig.~\ref{fig:architecture}
The encoder maps the data to a $d_{model}$ vector in the input layer, followed by positional encoding and four transformer specific encoder layers, consisting of self-attention, feed-forward and normalization each.
The decoder follows the same structure with two differences.
The transformer specific decoder layers consist of another self-attention layer, performing attention over the encoder output.
Also the output of the four decoder layers is passed into a fully connected linear layer, mapping the output to the dimension of the encoder input.

For training, they use US country and state-level ILI data to construct labeled data from weekly sliding windows.
In each window, the first $10$ records define the features while the next $4$ records define the labels.
The feature gets passed into the encoder, the label gets passed into decoder.
For the label, only a subsequence gets passed at once.
It begins with the last record passed into the encoder.
For a label-sequence of $l_i,\mathellipsis,l_{i+j}$ the decoder is expected to output $l_{i+1},\mathellipsis,l_{i+j+1}$.
A lookahead-mask is applied during training.
The model gets fitted using the Adam optimizer, with a scheduled learning rate of $5000$ warmup steps. %TODO: needs ref?
Dropout techniques are used for regularization.
\begin{figure}%[!htb]
    \centering
    \includegraphics[width=\linewidth,height=3cm,keepaspectratio]{LaTeX_demo_bibtex/figures/architecture.png}
    \caption{General architecture of transformer}
    \label{fig:architecture}
\end{figure}
~\\
\citet{wu_2020} perform experiments only using ILI data, as well as extracted features and time-delay embedding.
The transformer gets compared with ARIMA, LSTM and Seq2Seq models. %TODO: needs ref?
Performance gets measured using pearson correlation and root mean squeared error.
They report that the correlation coefficients are slightly better for transformers compared to other deep learning approaches.
The RMSE decreases up to $27\%$ and they interpret that transformers show better forecasting performance as well as improved generalization capability.
find transformers to achieve state of the art results and surpass the performance of many existing models.

Finally, \citet{wu_2020} conclude that the transformer approach to forecasting time series is able to learn complex dependencies from non-linear dynamic systems, and should be applicable to other settings.
Based on their experiments, they expect the model to adapt well to multivariate time-series and hypothesize that the approach could be generalized to spatio-temporal space.

\section{Application to Plant-Based Sensor Forecasting}
% \subsection{Problem Statement}
% \begin{itemize}
%     \item challenges posed by sparse sensor data.
%     \item Describe how they differ / resemble ILI data
% \end{itemize}
The transformer model for time series forecasting proposed by \citet{wu_2020} should now be reimplemented and applied to plant-based sensors.
The observed plants are held in controlled conditions and measured by [EXTERNAL DEVICE] and PhytoNode. %TODO: has ref?
Disturbances into the system are introduced artificially and the devices capture the plants reaction to the disturbance.
A dataset consists of measurements with one type of disturbance. 
We study a dataset where wind gets introduced and a dataset where temperature gets changed. %TODO: are they changed simultaneously or are those extra sessions? it seems that the wind and temperature disturbance get introduces simultaneously
An Example for the datasets used is provided in Fig.~\ref{fig:example_datasets}.

\subsection{Data Preprocessing}
% \begin{itemize}
% \item Data shape
% \item Preprocessing steps
% \end{itemize}
A dataset consists of records of variables taken at a specific timestamp in intervals of 1.5 seconds. 
We consider two variables measuring differential potential and one variable measuring transpiration.
To achieve a consistent interval, we aggregate multiple records by mean to a frequency of one timestamp every 5~s.
If there is not enough data to aggregate for a specific timestamp, the record is marked as invalid.
Then, the data gets min-max scaled to the interval $[0,1]$.

We define windows onto the time-series of length $M+N$.
The window is divided into first a feature vector of $M$ consecutive records, followed by the label vector of length $N$.
The windows get checked for validity and only windows with no invalid record get used.
This way, larger gaps between two timestamps in the original data are discarded.

\subsection{Transformers Model}
% \begin{itemize}
% \item Description of adapted transformers model
% \item Modifications and adjustments
% \item Solution provided by the model for Mathematical problem
% \end{itemize}
The implemented model closely aligns with the details presented at Section~\ref{sec:ili-prevalence-case}.

~\\
Let $W$ be a time window consisting of elements $w_i = (w_1,\mathellipsis,w_k)$, with $k$ being the number of feature dimensions.
Let $F =(w_1,\mathellipsis,w_j)$ be the features for an index $j$.
Let $L = (w_a, \mathellipsis, w_b)$ be the labels for two indices $a~<~b$.
Then, given the encoding $e_T$ of $F$, the transformer $T$ to be implemented should compute $T_{e_T}(L) = (w_{a+1},\mathellipsis,w_{b+1})$.

~\\
The model is implemented in python using pytorch\footnote{The repository is provided online \citep{jandeleit_2023_plant_based_transformers}.}. %TODO: ref?
It is implemented as four modules:
\begin{enumerate}
    \item Positional Encoding: An implementation of positional encoding module applying sine and cosine encoding to the input.
    \item Encoder: Encoder part of the transformer, containing input layer, positional encoding layer and 4 transformer encoding layers.
    \item Decoder: Decoder part of the transformer, containing the input layer, 4 decoder layers, output layer.
    \item Transformer: Actual transformer, containing encoder, decoder and the embedded representation of the encoder input.
\end{enumerate}

\subsection{Model Training}
As a neural network, transformer models need to be trained to compute the desired output.
\citet{wu_2020} does not provide details on training.
We employ a training strategy based on \citet{Vaswani2017} and include the details given by \citet{wu_2020}, i.e., masking. %TODO: others?

We first precompute the indices of valid time windows.
As the resulting dataset can be large, we do not train on the entire dataset to save time.
Instead, we train for $\epsilon$ iterations and draw a random window from the valid time windows each time.
The feature gets encoded by the encoder part of the transformer.
The label $(l_1,\mathellipsis,l_N)$ gets combined with the last element $f_M$ of the feature part into the input vector $v = (f_M,l_1,\mathellipsis,l_N)$ to the transformer decoder.
$v$ does not get passed to the transformer decoder as is. 
Instead, we iterate over $v$ and only take the first $i$ elements.
The output of the decoder then corresponds to the labels $l_1,\mathellipsis,l_{i}$.
The mean squared error between the output and those labels is defining the loss of that iteration of $i$ elements.
The weights then get updated using backpropagation and the adam optimizer. % TODO: reference to adam

\section{Experimental Evaluation}

% \subsection{Experiments}
\begin{itemize}
\item Experiments
\item Parameter and loss choice
\item Evaluation criteria
    \item present results of experiments
    \item performance on plant-based sensors vs ili
    \item discuss relevant findings
\end{itemize}

\paragraph{TODO}
We conduct an experiment with the parameters $T_d=16,T_h=16,\epsilon = 10000,N=12,N=10$, learning rate $\gamma = 0.001$ on the first $1000$ rows of the transpiration column in the temperature dataset, aggregated to a timefrequency $\Delta t=5~s$ by mean. %TODO: adam \alpha \beta
Each submodule uses $0.2$ dropout.
%TODO: describe experiment with inference
\begin{figure}%[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{LaTeX_demo_bibtex/figures/img_transpiration_reconstruct.png}
    \caption{Example inference}
    \label{fig:experiment_example_inference}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{LaTeX_demo_bibtex/figures/img_transpiration_losses.png}
    \caption{Losses during experiment}
    \label{fig:experiment_losses}
    \end{subfigure}

    \caption{Results of experiment}\label{fig:exp_results}
\end{figure}
% \subsection{Results}

The experiment shows acceptable results. 
The best lost achieved during training is $4.8*10^{-8}$.
The complete run is displayed in Fig.~\ref{fig:experiment_losses}.
Figure~\ref{fig:experiment_example_inference} shows an exemplary reconstruction of predicting $20$~steps ahead.

\section{Conclusion}
\begin{itemize}
    \item Main Contributions and findings
    \item Effectiveness of ts-transformers for task
    \item implications for further research
    \item ts-transformers a solution?
\end{itemize}



%\bibliographystyle{plain}
%\bibliographystyle{abbrv}
%\bibliographystyle{unsrt}

%\bibliographystyle{plainnat}
%\bibliographystyle{abbrvnat}
%\bibliographystyle{unsrtnat}
\bibliographystyle{elsart-harv}


\bibliography{jabref_cleaned}


\end{document}